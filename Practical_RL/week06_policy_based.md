- [Тренировки. Лекция 5: Современные методы обучения с подкреплением. Advantage actor critic, RLHF](https://www.youtube.com/watch?v=1zhehsSpV9Q&list=PLXtiZNKIobF6DmJc7MgCN3WTAPUZUJKmm&index=4)
- [Practical RL: Policy gradient methods](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based)
- [Policy Gradient – Федор Ратников](https://disk.yandex.ru/i/yPIPkO_f3TPsNK)
# 1. Мотивация

1) Как человек выбирает действия
2) Approximation error

$Q(s_0, a_0)$ | 1 | 1 | 2 |
$Q(s_0, a_1)$  | 2 | 2 | 1 |
$Q(s_1, a_0)$  | 3 | 3 | 3 |
$Q(s_1, a_1)$   | 100 | 50 | 100 |

С точки зрения MSE более правильным будет второе решение. 
Однако, если мы хотим повышать мат ожидание награды, то ...

Оценивать 

Хотим напрямую выучивать политику $\pi(a|s)$ в надежде что так будет проще решать некоторые задачи. 
# 2. Политика

## 2.1. Детерминированная и стохастическая политика

Вспомним что такое политика в RL

Политика — это “способ выбирать действие”.

$$\pi(a|s) = \text{вероятность выбрать действие } a \text{ в состоянии } s$$

Ранее мы рассматривали детерминированную политику $a = \pi_{\theta}(s)$, когда политика выдает ровно одно действие (например, $\arg \max Q(s,a)$).  

Стохастическая политика  $a \sim \pi_{\theta}(a|s)$ задаёт распределение действий (например, $\varepsilon$-greedy или softmax). 

Можно заметить, что с одной стороны, **стохастическая политика обобщает детерменистическую**. С другой стороны, **существуют задачи, где стохастическая политика оказывается оптимальнее детерменистической** (например, в игре "Камень, ножницы, бумага" оптимальной окажется случайная равновероятная политика). И иногда политика в виде вероятностей просто может оказаться более удобной. 

По этим причинам далее будем использовать именно cтохастическую политику.
## 2.2. Архитектура для стохастической политики

Когда количество действий ограничено, очевидно, что выходной слой нейронной сети должен возвращать вектор логитов размером $|A|$ (число возможных действий), и применять softmax для преобразования в распределение вероятностей.

$$\pi_{\theta}​(a|s) = softmax(Wh_{\theta}​(s))$$

В случае, когда количество действий не ограничено, политика должна выучивать параметры вероятностного распределения:
- Нормальное распределение (Gaussian): $a \sim \mathcal{N}(\mu_\theta(s), \sigma_\theta(s))$
- Смесь нормальных распределений (Mixture of Gaussians): $\pi(a|s) = \sum_{k=1}^{K} ​w_k ​\mathcal{N}(\mu_k​(s), \sigma_k​(s)^2)$
- Бетта распределение (Beta): $a \sim Beta(\alpha_{\theta}​(s), \betaθ_{\theta}(s))$

1. Value based методы – выучивают оптимальные дейтсвия косвенно через полезности  $Q_{\theta}(s,a)$ или $V_{\theta}(s)$.
	infer policy $a = argmax_a Q_{\theta}(s,a)$

2. Policy based методы – минуя промежуточные решения
	Явно выучивает политику $\pi_{\theta}(a|s)$ или $\pi_{theta}(s) \rightarrow a$
	Implicitly maximaze reward over policy

# 3. Оптимизация политики

Прежде, напомню, что мы пытаемся максимизировать один из функционалов:

1.  $J = \mathbb{E}_{\substack{s \sim p(s) \\ a \sim \pi_{\theta}(a|s)}} R(s,a,s',a',...)$ – математическое ожидание награды.
2. $J = \mathbb{E}_{\substack{s \sim p(s) \\ a \sim \pi_{\theta}(a|s)}} G(s,a)$, где $G(s,a) = r + \gamma \cdot G(s', a')$ – математическое ожидание **дисконтированной** награды.

Далее для простоты рассуждений:
1. Будем максимизировать первый случай, а именно математическое ожидание награды. 
2. Рассмотрим одношаговый процесс $s \rightarrow a \rightarrow r$.

Тогда оптимизируемый функционал для непрерывных действий и состояний будет иметь следующий вид:
$$J =
\underset{\substack{s \sim p(s) \\ a \sim \pi_{\theta}(s|a)}}{\mathbb{E}}
R(s,a,s',a',...) =
\int_s p(s) \int_a \pi_{\theta}(a|s) R(s,a) \mathrm{d}a \mathrm{d}s$$
где $p(s)$ – вероятность посещений состояния $s$ (может зависеть от политики), $\pi_{\theta}$(a|s) – вероятность выбрать действие $a$ в состоянии $s$ (политика), $R(s,a)$ – награда одношаговой траектории.

$\int_s p(s) ... ds$ - интеграл по реальному миру
 $\int_a \pi_{\theta}(a|s) R(s,a) da$ - интеграл по нейронной сети 

Мы хотим оптимизировать $\pi_{\theta}(a|s)$ чтобы максимизировать $J$.

Оптимизировать, конечно, хотелось бы градиентным спуском и для этого надо понять как продифференцировать $J$.

## 3.1. Метод Монте-Карло

Возникает идея оценить случайную величину $J$ методом Монте-Карло (проиграть $N$ сессий и усреднить награду) и продифферинцировать Монте-Карло по $\theta$:

$$ J \approx \frac{1}{N} \sum_{i=0}^{N} \sum_{s,a \in z_i} R(s,a)
\underset{\substack{\text{1-step} \\ \text{process}}}{=}
\frac{1}{N} \sum_{i=0}^{N} R(s,a)$$

Однако, заметим, что при замене интеграла на выборочное среднее пропадают параметры политики $\theta$. А значит, невозможно посчитать $\nabla_{\theta} J$.

## 3.2. Метод конечных разностей

Можно попытаться вместо градиента взять конечную разность:
$$\nabla J = \frac{J_{\theta + \varepsilon} - J_{\theta}}{\varepsilon}$$
И, если функция окажется достаточно гладкой, то это даже будет работать. 

Однако, такой подход имеет целый ряд недостатков:
1. Если J недостаточно гладкая (Липшецева), то ... ?
2. Очень долго работает в пространствах большой размерности. 
3. Скорее всего по 100 играм если мы изменит вес на 0.1 то качество изменится на величину которая сильно меньше чем шум

## 3.3. Стохастическая оптимизация (метод кросс-энтропии)

Ранне уже был рассмотрен простой эволюционный подход выучивания политики напрямую – [Метод кросс энтропии](https://habr.com/ru/articles/919556/#:~:text=%D0%B2%D0%B7%D0%B0%D0%B8%D0%BC%D0%BE%D0%B4%D0%B5%D0%B9%D1%81%D1%82%D0%B2%D0%B8%D1%8F%20%D1%81%D0%BB%D1%83%D1%87%D0%B0%D0%B9%D0%BD%D1%8B%D1%85%20%D0%B2%D0%B5%D0%BB%D0%B8%D1%87%D0%B8%D0%BD-,3.%20Cross%2DEntropy%20Method%2C%20CEM,-%D0%9A%D1%80%D0%BE%D1%81%D1%81%2D%D1%8D%D0%BD%D1%82%D1%80%D0%BE%D0%BF%D0%B8%D0%B9%D0%BD%D1%8B%D0%B9%20%D0%BC%D0%B5%D1%82%D0%BE%D0%B4).

Вспомним в чем его суть:

1. Инициализируем веса нейронной сети (политики) случайным образом $\theta_0 \leftarrow \text{random}$.
2. Основной цикл обучения:
	1. Генерируем N траекторий (сессий), играя эпизоды с текущей политикой.
	2. Выбираем $M < N$ лучших сессий с наибольшей наградой $\text{Elite} = [(s_0, a_0), (s_1, a_1), \ldots, (s_M, a_M)]$ 
	3. Обновляем параметры политики, чтобы она повторяла действия, которые встречались в лучших эпизодах: $\theta_{i+1} = \theta_{i} + \alpha \nabla \sum_i \log \pi_{\theta_i}(a_i|s_i) \cdot [s_i, a_i \in \text{Elite}]$

В CEM элита выбирается по эпизодам. 

Существует также **TD-версия Cross-Entropy Method (CEM)**, в которой элита выбирается **не по целым эпизодам, а по состояниям**. В этом подходе “элитной” считается не вся траектория, а лишь конкретная пара _(s, a)_, если она приводит к высокому будущему возврату _G(s, a)_. Такой способ отбора оказывается особенно полезным в ситуациях, когда эпизод в целом оказался неудачным, но отдельные решения внутри него были оптимальными. В отличие от классического CEM, где плохая траектория отбрасывается целиком, TD-CEM сохраняет ценные фрагменты поведения. Благодаря этому обучение становится **быстрее, стабильнее и менее зависимым от общей удачи/неудачи эпизода**, поскольку полезная информация не теряется.

Проблема если в среде где есть случайность то выбираем случаные действия чем боле правильные

Проблема что НЕ элитные сессии, выкидываем большое количество опыта

## 3.4. Дифференцирование интеграла

$$J = \int_s p(s) \int_a \pi_{\theta}(a|s) R(s,a) \mathrm{d}a \mathrm{d}s$$
Мы хотим аналитически выписать его градиент, чтобы затем заменить интегралы на выборочные средние (т.е. приблизить методом Монте-Карло):

$$[1] \quad \nabla J = \int_s p(s) \int_a \nabla \pi_{\theta}(a|s) R(s,a) \mathrm{d}a \mathrm{d}s$$

Однако, градиент плотности $\nabla \pi_{\theta}(a|s)$ не будет обладать свойствами вероятностного распределения (например, может стать отрицательной или больше единицы). Поэтому его невозможно использовать как вес при оценке математического ожидания сэмплами. А значит, мы опять не может оценить $\nabla J$.

Для того чтобы это исправить используют логарифмический трюк (**logderivative trick**):
Известно, что 
$$\nabla \log \pi(z) = \frac{1}{\pi(z)} \cdot \nabla \pi(z) 
\quad \text{,} \quad 
\pi(z) > 0$$ Тогда можно выразить градиент политики:  
$$[2] \quad \pi(x) \cdot \nabla \log \pi(z) = \nabla \pi(z)$$

Это тождество просто переписывает `градиент плотности` в виде `плотность × градиент лог-плотности`. И вот это уже важно: в левой части стоит сама плотность $\pi$, которую можно использовать как вес при семплировании.

Применим логарифмический трюк $[2]$ к нашем $[1]$:

$$\nabla J = \int_s p(s) \int_a {\color{red}{\nabla \pi_{\theta}(a|s)}} R(s,a) \mathrm{d}a \mathrm{d}s = \int_s p(s) \int_a 
{\color{red}{\pi_{\theta}(a|s) \nabla \log \pi_{\theta}(a|s)}} 
R(s,a) \mathrm{d}a \mathrm{d}s$$

Данная формула носит называется Policy Gradient и с ее помощью можно оценить $\nabla J$ методом Монте-Карло (Monte Carlo Policy Gradient): $$\nabla J = \frac{1}{N} \sum^{N}_{i=0} \nabla \log \pi_{\theta}(a|s) \cdot R(s,a)$$Остается обновить параметры политики градиентным подъемом: $\theta_{i+1} \leftarrow \theta_i + \alpha \nabla J$.

Так мы получили алгоритм REINFORCE для одношаговой игры:
1. Инициализируем веса нейронной сети случайным образом $\theta_0 \leftarrow \text{random}$.
2. Основной цикл обучения:
	1. Семплируем N сессий 
	2. Вычисляем Монте-Карло оценку: $\nabla J = \frac{1}{N} \sum^{N}_{i=0} \nabla \log \pi_{\theta}(a|s) \cdot R(s,a)$
	3. Обновляем параметры: $\theta_{i+1} \leftarrow \theta_i + \alpha \nabla J$

## 3.5.  Discounted reward case

[Policy Gradient Methods for Reinforcement Learning with Function Approximation](https://proceedings.neurips.cc/paper_files/paper/1999/file/464d828b85b0bed98e80ade0a5c43b0f-Paper.pdf)

В многшаговой постановке агент получает не одну награду, а последовательность наград. Поэтому вместо мгновенной награды рассматривают дисконтированную сумму будущих наград:
$$G_t = r + \gamma \cdot G(s', a')$$
Тогда оптимизируемым функционалом будет:
$$J = 
\underset{\substack{s \sim p(s) \\ a \sim \pi_{\theta}(a|s)}}{\mathbb{E}}
G(s,a) \rightarrow max_{\pi}$$
Аналогично, применим к градиенту логарифмический трюк:
$$\nabla J =
\int_s p(s) \int_a \nabla \pi_{\theta}(a|s) 
{\color{red} Q(s,a)}
\mathrm{d}a \mathrm{d}s 
\underset{\substack{\text{logderivative} \\ \text{trick}}}{=}
\int_s p(s) \int_a 
\pi_{\theta}(a|s) 
\nabla \log \pi_{\theta}(a|s)
{\color{red} Q(s,a)}
\mathrm{d}a \mathrm{d}s$$
Монте-Карло оценка градиента:
$$\nabla J = \frac{1}{N} \sum_{i=0}^{N} \sum_{a \in z_i} \nabla \log \pi_{\theta}(a|s) \cdot Q(s,a)$$
Классический алгоритм REINFORCE (1992):
1. Инициализируем веса нейронной сети случайным образом $\theta_0 \leftarrow \text{random}$.
2. Основной цикл обучения:
	1. Сэмплируем N эпизодов под текущей политикой $\pi(a|s)$, где каждый запуск даёт нам **траекторию** (эпизод): $z_i​=(s_0​,a_0​,r_0​,s_1​,a_1​,r_1​,…,s_T​,a_T​,r_T​)$
	2. Вычисляем оценку градиента целевой функции $J$: $\nabla J = \frac{1}{N} \sum^{N}_{i=0} \nabla \log \pi_{\theta}(a|s) \cdot R(s,a)$
	3. Обновляем параметры политики (gradient ascent): $\theta_{i+1} \leftarrow \theta_i + \alpha \nabla J$

 Важно итеративно считать $Q(s,a)$ начиная с конца для оптимизации вычислений. В противном случае это может занять слишком много времени. 

Еще несколько замечаний для алгоритма REINFORCE:
1. Это on-policy алгоритм (обучение происходит по текущей политике).
2. Работает быстрее, чем ванильный Q-learning.
3. Работает с любыми оптимизаторами (Adam, RMSProp).
4. Совместим с любыми архитектурами нейросетей (ConvNet, RNN, Transformer).
5. Концептуально похож на обучение с учителем.
## 3.6. Supervised Learning & Policy Gradient

Вспомним, что в задачах обучения с учителем (Supervised Learning), мы пытались максимизировать логарифм функции правдоподобия (log likelihood, llh). То есть найти такое $\theta$, для которого вероятность выбора действий $a={a_1,…,a_N}$ была бы максимальной. 

[4.1. Вероятностный подход в ML](https://education.yandex.ru/handbook/ml/article/veroyatnostnyj-podhod-v-ml)

$$llh = \sum \log \pi_{\theta}(a_{opt}|s, \theta) \rightarrow max_{\theta}$$
Градиент логарифм правдоподобия:
$$\nabla llh = 
\underset{s,a \sim D}{\mathbb{E}}
\nabla \log \pi_{\theta}(a_{opt}|s, \theta)$$
Градиент математического ожидания суммарной дисконтированной награды:
$$\nabla J = 
\underset{\substack{s \sim d(s) \\ a \sim \pi(a|obs(s))}}{\mathbb{E}}
\nabla \log \pi(a|s) Q(s,a)$$
Не трудно заметиться, что $\nabla llh$  и $\nabla J$ очень похожи.

Грубо говоря в $\nabla llh$ сидит награда $Q(s,a)$ в виде индикаторной функции которая единице если объекты принадлежат обучающей выборке, а в противном случае ноль:
$$\nabla llh = 
\underset{s,a \sim D}{\mathbb{E}}
\nabla \log \pi_{\theta}(a_{opt}|s, \theta) {\color{red}{Q(s,a)}}$$
$$Q(s,a) = \mathbb{I}(s, a) = \begin{cases} 
1 & \text{if } s, a \in D \\
0 & \text{else}
\end{cases}$$
Таким образом мы говорим, что все примеры из датасета $D$ одинаково важны. 

Максимизируя математическое ожидание награды в RL мы можем положить любую интересующую нас функцию награды (метрику или обученную модель rewordа), тем самым придав разную важность примерам. 

$$\nabla J = 
\underset{\substack{s \sim d(s) \\ a \sim \pi(a|obs(s))}}{\mathbb{E}}
\nabla \log \pi(a|s) 
{\color{red}{R(s,a)}}$$

Таким образом, задачу supervised learning можно рассматривать как частный частный случай оптимизации ожидаемой награды, где награда даётся за правильный ответ. 

Преимущества supervised learning заключаются в том, что обучение в этой постановке гораздо проще: нет траекторий, нет стохастических политик, и каждая пара «вход–ответ» рассматривается независимо. Благодаря этому обучение быстро сходится, градиенты более стабильны, а оценка ошибки обладает низкой дисперсией.
Основные недостатки — необходимость в хорошо размеченной выборке и **проблема смещения распределений (distribution shift)**: модель обучается на одном распределении данных, а применять её приходится на другом, что приводит к деградации качества.

В обучении с подкреплением данные разметки не требуются: достаточно иметь состояния $s$, действия $a$ и сигнал награды $r(s,a)$. Агент сам генерирует данные в процессе взаимодействия со средой, поэтому **отсутствует классический distribution shift** — распределение данных всегда соответствует текущей политике.
К недостаткам относятся **проблема холодного старта**, когда агент поначалу действует почти случайно и получает слабый обучающий сигнал, а также **большой разброс оценок**: стохастичность среды и траекторий приводит к высокой дисперсии градиентов и нестабильному обучению.

На практике модель сначала предварительно обучают supervised-методами, чтобы задать хорошую инициализацию, а затем дооптимизируют с помощью RL.
# 4. Улучшения

## 4.1. Baselines

Рассмотрим простейшую среду — одношаговую игру, в которой агенту доступно всего два действия: выбрать левую или правую кнопку. Каждое действие немедленно приводит к завершению эпизода и выдаёт фиксированную награду: награда за левую кнопку $+1$; награда за правую кнопку $-1$. Таким образом, оптимальная стратегия очевидна: вероятность того что агент выберет левую кнопку будет расти, а вероятность выбора правой кнопки будет падать.

![[Pasted image 20251128032745.png]]

Немного поменяем условия задачи выше: прибавим к награде $+100$. Теперь награда за левую кнопку $+101$, а награда за правую кнопку $+99$. Не смотря на то что уловие задачи не поменялось, вероятность выбора и левой и правой кнопки будет возрастать, хоть левой немного сильнее. 

![[Pasted image 20251128032933.png]]

Раз обучение так отличается от того что мы прибавили константу, то напрашивается вывод, что для более эффективного обучения надо вычесть какую-то константу. 

Можно выучить некоторую  $b(s)$ и вычесть его из $Q(s,a)$. 
$$
\nabla J = 
\underset{\substack{s \sim p(s) \\ a \sim \pi(a|s)}}{\mathbb{E}}
\nabla \log \pi(a|s) (Q(s,a) - {\color{red}{b(s)}})
$$
Опустив все формулы, сразу скажу вывод: baseline **не меняет математическое ожидание** градиента (оно остаётся корректным), но при этом **влияет на его дисперсию**. Если подобрать $b(s)$ удачно, можно значительно уменьшить разброс Монте-Карло оценок и заметно ускорить обучение. При этом, **известный практический факт**, что $b(s)$ не учится. Но можно показать, что если вместо $b(s)$ использовать математическое ожидание награды, то дисперсия будет меньше (но не минимальной).  

On variance of policy gradient and optimal baselines: [article](https://papers.nips.cc/paper/4264-analysis-and-improvement-of-policy-gradient-estimation.pdf), another [article](https://arxiv.org/pdf/1301.2315.pdf)
 [Practical RL: Policy gradient methods](https://github.com/yandexdataschool/Practical_RL/tree/master/week06_policy_based)

b(s) называется baseline. В качестве baseline можно брать функцию состояния $V(s)$ (но не только). 

$$b(s)=V(s)=E[G_t​∣s_t​=s]$$
## 4.2. Actor-critic

Алгоритмы, которые учат не только вероятность действий $\pi(a|s)$, но и оценку полезности состояния V(s), называются **actor–critic**.
![[Pasted image 20251128175742.png]]

В таких моделях оценка градиента политики: 
$$\nabla J = 
\frac{1}{N} \sum_{i=0}^{N} \sum_{s,a \in z_i} 
\nabla \log \pi(a|s) 
( {\color{red}{Q(s,a) - V(s)}})$$
использует **advantage** — насколько действие лучше или хуже ожидаемого результата в этом состоянии. Если действие хуже обычного — градиент уменьшает вероятность. Если лучше — увеличивает.

Напрямую посчитать $Q(s,a)$ можно только проиграв всю траекторию до конца. Однако, если мы знаем $V(s)$ и кортеж $(s,a,r,s')$ можно оценить $Q(s,a)$ по формуле: $Q(s,a) = r + \gamma V(s')$.

Обозначим разницу $Q(s,a) - V(s)$ как $A(s,a)$ (от слова advantage): $A(s,a) = Q(s,a) - V(s) = r + \gamma V(s') - V(s)$.

Модель, которая учится делать действия называется Actor. 
Ее функция потерь: $J_{actor} = \int_s p(s) \int_a \pi(s|a) A(s,a) da ds \rightarrow \max$
И стохастическая оценка: 
$$\nabla J_{actor} \approx \frac{1}{N} \sum_{i=0}^{N} \sum_{s,a \in z_i} \nabla \log \pi (a|s) A(s,a)$$
Модель, которая оценивает качества состояний/действий (_критикует_ действие) называется Critic. Чтобы обучить Critic $V(s)$  пользуемся идеей из Q-learning: 
$$L_{critic} \approx \frac{1}{N} \sum_{i=0}^{N} \sum_{s,a \in z_i} \big( V(s) - \left[ r + \gamma \cdot V(s')\right] \big) ^2 \rightarrow \min$$

![[Pasted image 20251128050135.png]]

Алгоритм:

Про коэффициенты лоссов

# 5. Proximal Policy Optimization (PPO)


# 5. GRPO


